{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from xml.dom import minidom\n",
    "import string\n",
    "from string import punctuation\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    from string import maketrans\n",
    "import re\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Predictor Launched)\n",
      "        id      file  earnings: 0 no/ 1 yes\n",
      "0        1     1.xml                      0\n",
      "1        2     2.xml                      0\n",
      "2        3     3.xml                      0\n",
      "3        4     4.xml                      0\n",
      "4        5     5.xml                      0\n",
      "5        6     6.xml                      0\n",
      "6        7     7.xml                      0\n",
      "7        8     8.xml                      0\n",
      "8        9     9.xml                      1\n",
      "9       10    10.xml                      0\n",
      "10      11    11.xml                      1\n",
      "11      12    12.xml                      1\n",
      "12      13    13.xml                      1\n",
      "13      14    14.xml                      1\n",
      "14      15    15.xml                      0\n",
      "15      16    16.xml                      0\n",
      "16      17    17.xml                      0\n",
      "17      18    18.xml                      1\n",
      "18      19    19.xml                      0\n",
      "19      21    21.xml                      0\n",
      "20      22    22.xml                      0\n",
      "21      23    23.xml                      1\n",
      "22      24    24.xml                      1\n",
      "23      25    25.xml                      0\n",
      "24      26    26.xml                      0\n",
      "25      27    27.xml                      1\n",
      "26      28    28.xml                      0\n",
      "27      29    29.xml                      0\n",
      "28      30    30.xml                      0\n",
      "29      31    31.xml                      0\n",
      "...    ...       ...                    ...\n",
      "4770  4971  4971.xml                      0\n",
      "4771  4972  4972.xml                      1\n",
      "4772  4973  4973.xml                      0\n",
      "4773  4974  4974.xml                      1\n",
      "4774  4975  4975.xml                      0\n",
      "4775  4976  4976.xml                      0\n",
      "4776  4977  4977.xml                      0\n",
      "4777  4978  4978.xml                      0\n",
      "4778  4979  4979.xml                      0\n",
      "4779  4980  4980.xml                      0\n",
      "4780  4981  4981.xml                      0\n",
      "4781  4982  4982.xml                      0\n",
      "4782  4983  4983.xml                      0\n",
      "4783  4984  4984.xml                      0\n",
      "4784  4985  4985.xml                      0\n",
      "4785  4986  4986.xml                      0\n",
      "4786  4987  4987.xml                      0\n",
      "4787  4988  4988.xml                      0\n",
      "4788  4989  4989.xml                      0\n",
      "4789  4990  4990.xml                      1\n",
      "4790  4991  4991.xml                      1\n",
      "4791  4992  4992.xml                      0\n",
      "4792  4993  4993.xml                      0\n",
      "4793  4994  4994.xml                      0\n",
      "4794  4995  4995.xml                      0\n",
      "4795  4996  4996.xml                      0\n",
      "4796  4997  4997.xml                      0\n",
      "4797  4998  4998.xml                      1\n",
      "4798  4999  4999.xml                      1\n",
      "4799  5000  5000.xml                      0\n",
      "\n",
      "[4800 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "class ArticlePredictorBase:\n",
    "    def __init__(self, path_to_csv_train, path_to_csv_test):\n",
    "        self.train_csv_file = path_to_csv_train\n",
    "        self.test_csv_file = path_to_csv_test\n",
    "        \n",
    "    def execute(self):\n",
    "        print(\"(Predictor Launched)\")\n",
    "        try:\n",
    "            self.train_csv = pd.read_csv(train_csv_file,delimiter=',')\n",
    "            self.test_csv = pd.read_csv(test_csv_file,delimiter=',')\n",
    "        except:\n",
    "            print(\"Error while importing csv files\") \n",
    "            \n",
    "        print(self.train_csv)\n",
    "    \n",
    "class ArticlePrediction:\n",
    "    def __init__(self, path_to_xml):\n",
    "        self.xml_file = path_to_xml\n",
    "    \n",
    "predictor = ArticlePredictorBase(\"project/project/train.csv\",\"project/project/test.csv\")\n",
    "predictor.execute()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of 1 file\n",
    "\n",
    "We test here with 1.xml data file\n",
    "\n",
    "### Importation of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('project/project/data/1.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "\n",
    "#Verification\n",
    "\n",
    "if len(itemlist) != 1:\n",
    "    print('Error: XML file invalid')\n",
    "    sys.exit(0)\n",
    "\n",
    "text = itemlist[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 tokens BEFORE stop words removing\n",
      "248 tokens AFTER stop words removing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Preprocess\n",
    "text = text.lower()    \n",
    "text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "#Preprocess ideas : https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "    \n",
    "#Tokenization    \n",
    "tokens = word_tokenize(text)\n",
    "print(str(len(tokens)) + \" tokens BEFORE stop words removing\")\n",
    "tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print(str(len(tokens)) + \" tokens AFTER stop words removing\")\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "#stemmer= PorterStemmer()\n",
    "#tokens = set(map(stemmer.stem,tokens))\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dlrs', 14)\n",
      "('new', 9)\n",
      "('york', 8)\n",
      "('sales', 7)\n",
      "('times', 7)\n",
      "('cocoa', 6)\n",
      "('comissaria', 5)\n",
      "('smith', 5)\n",
      "('said', 5)\n",
      "('bags', 5)\n",
      "('mln', 5)\n",
      "('crop', 5)\n",
      "('bahia', 4)\n",
      "('february', 3)\n",
      "('total', 3)\n",
      "('ports', 3)\n",
      "('junejuly', 3)\n",
      "('augsept', 3)\n",
      "('marchapril', 3)\n",
      "('octdec', 3)\n",
      "('dec', 3)\n",
      "('week', 2)\n",
      "('temporao', 2)\n",
      "('period', 2)\n",
      "('year', 2)\n",
      "('arrivals', 2)\n",
      "('kilos', 2)\n",
      "('consignment', 2)\n",
      "('figures', 2)\n",
      "('farmers', 2)\n",
      "('shippers', 2)\n",
      "('sold', 2)\n",
      "('bean', 2)\n",
      "('shipment', 2)\n",
      "('limited', 2)\n",
      "('tonne', 2)\n",
      "('open', 2)\n",
      "('july', 2)\n",
      "('butter', 2)\n",
      "('sept', 2)\n",
      "('currency', 2)\n",
      "('areas', 2)\n",
      "('uruguay', 2)\n",
      "('showers', 1)\n",
      "('continued', 1)\n",
      "('zone', 1)\n",
      "('alleviating', 1)\n",
      "('drought', 1)\n",
      "('early', 1)\n",
      "('january', 1)\n",
      "('improving', 1)\n",
      "('prospects', 1)\n",
      "('coming', 1)\n",
      "('normal', 1)\n",
      "('humidity', 1)\n",
      "('levels', 1)\n",
      "('restored', 1)\n",
      "('weekly', 1)\n",
      "('review', 1)\n",
      "('dry', 1)\n",
      "('means', 1)\n",
      "('late', 1)\n",
      "('ended', 1)\n",
      "('making', 1)\n",
      "('cumulative', 1)\n",
      "('season', 1)\n",
      "('stage', 1)\n",
      "('delivered', 1)\n",
      "('earlier', 1)\n",
      "('included', 1)\n",
      "('doubt', 1)\n",
      "('old', 1)\n",
      "('available', 1)\n",
      "('harvesting', 1)\n",
      "('practically', 1)\n",
      "('come', 1)\n",
      "('end', 1)\n",
      "('estimates', 1)\n",
      "('standing', 1)\n",
      "('thousand', 1)\n",
      "('hands', 1)\n",
      "('middlemen', 1)\n",
      "('exporters', 1)\n",
      "('processors', 1)\n",
      "('doubts', 1)\n",
      "('fit', 1)\n",
      "('export', 1)\n",
      "('experiencing', 1)\n",
      "('dificulties', 1)\n",
      "('obtaining', 1)\n",
      "('superior', 1)\n",
      "('certificates', 1)\n",
      "('view', 1)\n",
      "('lower', 1)\n",
      "('quality', 1)\n",
      "('recent', 1)\n",
      "('weeks', 1)\n",
      "('good', 1)\n",
      "('held', 1)\n",
      "('spot', 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        print(word)\n",
    "        \n",
    "most_common_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
