{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator, re, string, codecs, nltk\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from xml.dom import minidom\n",
    "from string import punctuation\n",
    "from enum import Enum\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    from string import maketrans\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Test\n",
    "\n",
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import xgboost, textblob\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Launched)\n",
      "3600 elements in the set\n"
     ]
    }
   ],
   "source": [
    "class Type(Enum):\n",
    "    REAL_CSV_SETS = 1\n",
    "    TEST_ON_TRAINING_SET = 2\n",
    "\n",
    "class ArticlePredictorBase:\n",
    "    def __init__(self,test_type,path_to_csv_train,path_to_csv_test):\n",
    "        if Type.REAL_CSV_SETS == test_type:\n",
    "            \n",
    "            #Import of the test set\n",
    "            try:\n",
    "                test_csv_file = path_to_csv_test\n",
    "                test = pd.read_csv(test_csv_file,delimiter=',')\n",
    "                self.test_x = test.iloc[:,1]\n",
    "                self.test_x = test.iloc[:,2]\n",
    "            except:\n",
    "                print(\"Error while importing testing set csv file\") \n",
    "                self.type = None\n",
    "            \n",
    "            #Import of the train set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                train = pd.read_csv(train_csv_file,delimiter=',')\n",
    "                self.train_x = test.iloc[:,1]\n",
    "                self.train_y = test.iloc[:,2]\n",
    "            except:\n",
    "                print(\"Error while importing training set csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            self.type = test_type\n",
    "                \n",
    "        elif Type.TEST_ON_TRAINING_SET == test_type:\n",
    "            \n",
    "            #Import of the train/testing set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                data = pd.read_csv(train_csv_file,delimiter=',')\n",
    "            except:\n",
    "                print(\"Error while importing the csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            self.type = test_type\n",
    "                \n",
    "            X = data.iloc[:,1]\n",
    "            y = data.iloc[:,2]\n",
    "            \n",
    "            self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(X, y)\n",
    "        else:\n",
    "            self.type = None\n",
    "            print('Error type')\n",
    "\n",
    "    def preprocess(self, set_x, set_y):\n",
    "        the_set = pd.DataFrame()\n",
    "        the_set['file_name'] = set_x\n",
    "        the_set['label'] = set_y\n",
    "        \n",
    "        for file in the_set['file_name']:\n",
    "            file_path = 'project/project/data/'+file\n",
    "            #excepted = train['label'][i] #Marche pas, super chiant d'accéder au label correspondant d'ici..\n",
    "            xmldoc = minidom.parse(file_path)\n",
    "            itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "            \n",
    "            if len(itemlist) != 1:\n",
    "                print('Error: XML file invalid')\n",
    "\n",
    "            if itemlist[0].childNodes == []: #If the text is empty in the article\n",
    "                continue\n",
    "                \n",
    "            text = itemlist[0].childNodes[0].nodeValue\n",
    "            \n",
    "            #if(itemlist[0].childNodes[0] is not None)\n",
    "            \n",
    "            \n",
    "            #TODO trouver comment accéder au set d'ici, pour y ajouter des champs\n",
    "            #TODO preprocess (accès au fichier, compute des trucs, ajouter des champs dans le set)\n",
    "            \n",
    "        return set_x, set_y\n",
    "            \n",
    "    def test(self):\n",
    "        print(\"(Testing Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            raise\n",
    "\n",
    "        test_x, test_y = self.preprocess(self.test_x, self.test_y)\n",
    "        #TODO testing\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"(Training Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            raise\n",
    "            \n",
    "        train_x, train_y = self.preprocess(self.train_x, self.train_y)\n",
    "        #TODO training\n",
    "            \n",
    "         \n",
    "#predictor = ArticlePredictorBase(Type.REAL_CSV_SETS,\"project/project/train.csv\",\"project/project/test.csv\")\n",
    "predictor = ArticlePredictorBase(Type.TEST_ON_TRAINING_SET,\"project/project/train.csv\",None)\n",
    "predictor.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of 1 file\n",
    "\n",
    "We test here with 1.xml data file\n",
    "\n",
    "### Importation of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('project/project/data/1.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "\n",
    "#Verification\n",
    "\n",
    "if len(itemlist) != 1:\n",
    "    print('Error: XML file invalid')\n",
    "    sys.exit(0)\n",
    "\n",
    "text = itemlist[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess\n",
    "text = text.lower()    \n",
    "text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "#Preprocess ideas : https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "     \n",
    "#Tokenization    \n",
    "tokens = word_tokenize(text)\n",
    "print(str(len(tokens)) + \" tokens BEFORE stop words removing\")\n",
    "tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print(str(len(tokens)) + \" tokens AFTER stop words removing\")\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "#stemmer= PorterStemmer()\n",
    "#tokens = set(map(stemmer.stem,tokens))\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        print(word)\n",
    "        \n",
    "most_common_terms(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
