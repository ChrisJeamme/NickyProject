{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator, re, string, codecs, nltk\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from xml.dom import minidom\n",
    "from string import punctuation\n",
    "from enum import Enum\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    from string import maketrans\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Test\n",
    "\n",
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import xgboost, textblob\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       1\n",
      "9       0\n",
      "10      1\n",
      "11      1\n",
      "12      1\n",
      "13      1\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      1\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      1\n",
      "22      1\n",
      "23      0\n",
      "24      0\n",
      "25      1\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "4770    0\n",
      "4771    1\n",
      "4772    0\n",
      "4773    1\n",
      "4774    0\n",
      "4775    0\n",
      "4776    0\n",
      "4777    0\n",
      "4778    0\n",
      "4779    0\n",
      "4780    0\n",
      "4781    0\n",
      "4782    0\n",
      "4783    0\n",
      "4784    0\n",
      "4785    0\n",
      "4786    0\n",
      "4787    0\n",
      "4788    0\n",
      "4789    1\n",
      "4790    1\n",
      "4791    0\n",
      "4792    0\n",
      "4793    0\n",
      "4794    0\n",
      "4795    0\n",
      "4796    0\n",
      "4797    1\n",
      "4798    1\n",
      "4799    0\n",
      "Name: earnings: 0 no/ 1 yes, Length: 4800, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Type(Enum):\n",
    "    REAL_CSV_SETS = 1\n",
    "    TEST_ON_TRAINING_SET = 2\n",
    "\n",
    "class ArticlePredictorBase:\n",
    "    def __init__(self,test_type,path_to_csv_train,path_to_csv_test):\n",
    "        if Type.REAL_CSV_SETS == test_type:\n",
    "            self.type = test_type\n",
    "            \n",
    "            #Import of the test set\n",
    "            try:\n",
    "                test_csv_file = path_to_csv_test\n",
    "                test = pd.read_csv(test_csv_file,delimiter=',')\n",
    "                #TODO self.valid_x, self.valid_y in this case\n",
    "                print(\"Not yet implemented\")\n",
    "            except:\n",
    "                print(\"Error while importing testing set csv file\") \n",
    "                self.type = None\n",
    "            \n",
    "            #Import of the train set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                train = pd.read_csv(train_csv_file,delimiter=',')\n",
    "                #TODO self.train_x, self.train_y in this case\n",
    "                print(\"Not yet implemented\")\n",
    "            except:\n",
    "                print(\"Error while importing training set csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "        elif Type.TEST_ON_TRAINING_SET == test_type:\n",
    "            self.type = test_type\n",
    "            \n",
    "            #Import of the train/testing set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                data = pd.read_csv(train_csv_file,delimiter=',')\n",
    "            except:\n",
    "                print(\"Error while importing the csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            #X = pd.get_dummies(data.iloc[:,3]) # On met quoi ici??\n",
    "            y = data.iloc[:,2]\n",
    "            \n",
    "            print(y)\n",
    "            \n",
    "            #print(pd.get_dummies(data.iloc[:,:3])) #test\n",
    "            #self.train_x, self.valid_x, self.train_y, self.valid_y = model_selection.train_test_split(theSet['text'], theSet['label'])\n",
    "        else:\n",
    "            self.type = None\n",
    "            print('Error type')\n",
    "\n",
    "    def test(self):\n",
    "        print(\"(Testing Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            return None\n",
    "        \n",
    "        test_x = self.test_x\n",
    "        test_y = self.test_y\n",
    "            \n",
    "    def train(self):\n",
    "        print(\"(Training Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            return None\n",
    "            \n",
    "        train_x = self.train_x\n",
    "        train_y = self.train_y\n",
    "         \n",
    "        # for i in range(0,train.size//3):\n",
    "        #    file_path = train['file'][i]\n",
    "        #    excepted = train['earnings: 0 no/ 1 yes'][i]\n",
    "        #    print(file_path)\n",
    "            #xmldoc = minidom.parse('project/project/'+file_path)\n",
    "    \n",
    "# predictor = ArticlePredictorBase(Type.REAL_CSV_SETS,\"project/project/train.csv\",\"project/project/test.csv\")\n",
    "predictor = ArticlePredictorBase(Type.TEST_ON_TRAINING_SET,\"project/project/train.csv\",\"project/project/test.csv\")\n",
    "# predictor.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of 1 file\n",
    "\n",
    "We test here with 1.xml data file\n",
    "\n",
    "### Importation of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('project/project/data/1.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "\n",
    "#Verification\n",
    "\n",
    "if len(itemlist) != 1:\n",
    "    print('Error: XML file invalid')\n",
    "    sys.exit(0)\n",
    "\n",
    "text = itemlist[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 tokens BEFORE stop words removing\n",
      "248 tokens AFTER stop words removing\n"
     ]
    }
   ],
   "source": [
    "#Preprocess\n",
    "text = text.lower()    \n",
    "text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "#Preprocess ideas : https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "     \n",
    "#Tokenization    \n",
    "tokens = word_tokenize(text)\n",
    "print(str(len(tokens)) + \" tokens BEFORE stop words removing\")\n",
    "tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print(str(len(tokens)) + \" tokens AFTER stop words removing\")\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "#stemmer= PorterStemmer()\n",
    "#tokens = set(map(stemmer.stem,tokens))\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dlrs', 14)\n",
      "('new', 9)\n",
      "('york', 8)\n",
      "('sales', 7)\n",
      "('times', 7)\n",
      "('cocoa', 6)\n",
      "('comissaria', 5)\n",
      "('smith', 5)\n",
      "('said', 5)\n",
      "('bags', 5)\n",
      "('mln', 5)\n",
      "('crop', 5)\n",
      "('bahia', 4)\n",
      "('february', 3)\n",
      "('total', 3)\n",
      "('ports', 3)\n",
      "('junejuly', 3)\n",
      "('augsept', 3)\n",
      "('marchapril', 3)\n",
      "('octdec', 3)\n",
      "('dec', 3)\n",
      "('week', 2)\n",
      "('temporao', 2)\n",
      "('period', 2)\n",
      "('year', 2)\n",
      "('arrivals', 2)\n",
      "('kilos', 2)\n",
      "('consignment', 2)\n",
      "('figures', 2)\n",
      "('farmers', 2)\n",
      "('shippers', 2)\n",
      "('sold', 2)\n",
      "('bean', 2)\n",
      "('shipment', 2)\n",
      "('limited', 2)\n",
      "('tonne', 2)\n",
      "('open', 2)\n",
      "('july', 2)\n",
      "('butter', 2)\n",
      "('sept', 2)\n",
      "('currency', 2)\n",
      "('areas', 2)\n",
      "('uruguay', 2)\n",
      "('showers', 1)\n",
      "('continued', 1)\n",
      "('zone', 1)\n",
      "('alleviating', 1)\n",
      "('drought', 1)\n",
      "('early', 1)\n",
      "('january', 1)\n",
      "('improving', 1)\n",
      "('prospects', 1)\n",
      "('coming', 1)\n",
      "('normal', 1)\n",
      "('humidity', 1)\n",
      "('levels', 1)\n",
      "('restored', 1)\n",
      "('weekly', 1)\n",
      "('review', 1)\n",
      "('dry', 1)\n",
      "('means', 1)\n",
      "('late', 1)\n",
      "('ended', 1)\n",
      "('making', 1)\n",
      "('cumulative', 1)\n",
      "('season', 1)\n",
      "('stage', 1)\n",
      "('delivered', 1)\n",
      "('earlier', 1)\n",
      "('included', 1)\n",
      "('doubt', 1)\n",
      "('old', 1)\n",
      "('available', 1)\n",
      "('harvesting', 1)\n",
      "('practically', 1)\n",
      "('come', 1)\n",
      "('end', 1)\n",
      "('estimates', 1)\n",
      "('standing', 1)\n",
      "('thousand', 1)\n",
      "('hands', 1)\n",
      "('middlemen', 1)\n",
      "('exporters', 1)\n",
      "('processors', 1)\n",
      "('doubts', 1)\n",
      "('fit', 1)\n",
      "('export', 1)\n",
      "('experiencing', 1)\n",
      "('dificulties', 1)\n",
      "('obtaining', 1)\n",
      "('superior', 1)\n",
      "('certificates', 1)\n",
      "('view', 1)\n",
      "('lower', 1)\n",
      "('quality', 1)\n",
      "('recent', 1)\n",
      "('weeks', 1)\n",
      "('good', 1)\n",
      "('held', 1)\n",
      "('spot', 1)\n",
      "('prices', 1)\n",
      "('rose', 1)\n",
      "('cruzados', 1)\n",
      "('arroba', 1)\n",
      "('reluctant', 1)\n",
      "('offer', 1)\n",
      "('nearby', 1)\n",
      "('booked', 1)\n",
      "('march', 1)\n",
      "('named', 1)\n",
      "('light', 1)\n",
      "('going', 1)\n",
      "('fob', 1)\n",
      "('routine', 1)\n",
      "('aprilmay', 1)\n",
      "('went', 1)\n",
      "('destinations', 1)\n",
      "('covertible', 1)\n",
      "('cake', 1)\n",
      "('registered', 1)\n",
      "('aug', 1)\n",
      "('buyers', 1)\n",
      "('argentina', 1)\n",
      "('convertible', 1)\n",
      "('liquor', 1)\n",
      "('selling', 1)\n",
      "('currently', 1)\n",
      "('estimated', 1)\n",
      "('final', 1)\n",
      "('expected', 1)\n",
      "('published', 1)\n",
      "('brazilian', 1)\n",
      "('trade', 1)\n",
      "('commission', 1)\n",
      "('carnival', 1)\n",
      "('ends', 1)\n",
      "('midday', 1)\n",
      "('reuter', 1)\n"
     ]
    }
   ],
   "source": [
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        print(word)\n",
    "        \n",
    "most_common_terms(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
