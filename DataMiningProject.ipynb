{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from xml.dom import minidom\n",
    "import string\n",
    "from string import punctuation\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    from string import maketrans\n",
    "import re\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code qui servira un jour ou l'autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation\n",
    "train_csv_file = \"project/project/train.csv\"\n",
    "train_csv = pd.read_csv(train_csv_file, delim_whitespace=True)\n",
    "\n",
    "test_csv_file = \"project/project/test.csv\"\n",
    "test_csv = pd.read_csv(test_csv_file, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of 1 file\n",
    "\n",
    "We test here with 1.xml data file\n",
    "\n",
    "### Importation of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('project/project/data/1.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "\n",
    "#Verification\n",
    "\n",
    "if len(itemlist) != 1:\n",
    "    print('Error: XML file invalid')\n",
    "    sys.exit(0)\n",
    "\n",
    "text = itemlist[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 tokens BEFORE stop words removing\n",
      "248 tokens AFTER stop words removing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Preprocess\n",
    "text = text.lower()    \n",
    "text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "#Preprocess ideas : https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "    \n",
    "#Tokenization    \n",
    "tokens = word_tokenize(text)\n",
    "print(str(len(tokens)) + \" tokens BEFORE stop words removing\")\n",
    "tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print(str(len(tokens)) + \" tokens AFTER stop words removing\")\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "#stemmer= PorterStemmer()\n",
    "#tokens = set(map(stemmer.stem,tokens))\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dlrs', 14)\n",
      "('new', 9)\n",
      "('york', 8)\n",
      "('sales', 7)\n",
      "('times', 7)\n",
      "('cocoa', 6)\n",
      "('comissaria', 5)\n",
      "('smith', 5)\n",
      "('said', 5)\n",
      "('bags', 5)\n",
      "('mln', 5)\n",
      "('crop', 5)\n",
      "('bahia', 4)\n",
      "('february', 3)\n",
      "('total', 3)\n",
      "('ports', 3)\n",
      "('junejuly', 3)\n",
      "('augsept', 3)\n",
      "('marchapril', 3)\n",
      "('octdec', 3)\n",
      "('dec', 3)\n",
      "('week', 2)\n",
      "('temporao', 2)\n",
      "('period', 2)\n",
      "('year', 2)\n",
      "('arrivals', 2)\n",
      "('kilos', 2)\n",
      "('consignment', 2)\n",
      "('figures', 2)\n",
      "('farmers', 2)\n",
      "('shippers', 2)\n",
      "('sold', 2)\n",
      "('bean', 2)\n",
      "('shipment', 2)\n",
      "('limited', 2)\n",
      "('tonne', 2)\n",
      "('open', 2)\n",
      "('july', 2)\n",
      "('butter', 2)\n",
      "('sept', 2)\n",
      "('currency', 2)\n",
      "('areas', 2)\n",
      "('uruguay', 2)\n",
      "('showers', 1)\n",
      "('continued', 1)\n",
      "('zone', 1)\n",
      "('alleviating', 1)\n",
      "('drought', 1)\n",
      "('early', 1)\n",
      "('january', 1)\n",
      "('improving', 1)\n",
      "('prospects', 1)\n",
      "('coming', 1)\n",
      "('normal', 1)\n",
      "('humidity', 1)\n",
      "('levels', 1)\n",
      "('restored', 1)\n",
      "('weekly', 1)\n",
      "('review', 1)\n",
      "('dry', 1)\n",
      "('means', 1)\n",
      "('late', 1)\n",
      "('ended', 1)\n",
      "('making', 1)\n",
      "('cumulative', 1)\n",
      "('season', 1)\n",
      "('stage', 1)\n",
      "('delivered', 1)\n",
      "('earlier', 1)\n",
      "('included', 1)\n",
      "('doubt', 1)\n",
      "('old', 1)\n",
      "('available', 1)\n",
      "('harvesting', 1)\n",
      "('practically', 1)\n",
      "('come', 1)\n",
      "('end', 1)\n",
      "('estimates', 1)\n",
      "('standing', 1)\n",
      "('thousand', 1)\n",
      "('hands', 1)\n",
      "('middlemen', 1)\n",
      "('exporters', 1)\n",
      "('processors', 1)\n",
      "('doubts', 1)\n",
      "('fit', 1)\n",
      "('export', 1)\n",
      "('experiencing', 1)\n",
      "('dificulties', 1)\n",
      "('obtaining', 1)\n",
      "('superior', 1)\n",
      "('certificates', 1)\n",
      "('view', 1)\n",
      "('lower', 1)\n",
      "('quality', 1)\n",
      "('recent', 1)\n",
      "('weeks', 1)\n",
      "('good', 1)\n",
      "('held', 1)\n",
      "('spot', 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        print(word)\n",
    "        \n",
    "most_common_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
