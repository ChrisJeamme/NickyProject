{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator, re, string, codecs, nltk\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from xml.dom import minidom\n",
    "from string import punctuation\n",
    "from enum import Enum\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    from string import maketrans\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Test\n",
    "\n",
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import xgboost, textblob\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Launched)\n",
      "3600 elements in the training set\n",
      "[('said', 2), ('change', 2), ('april', 2), ('unc', 1), ('board', 1), ('approved', 1), ('state', 1), ('incorporation', 1), ('delaware', 1), ('virginia', 1), ('subject', 1), ('approval', 1), ('shareholders', 1), ('annual', 1), ('meeting', 1), ('expects', 1), ('make', 1), ('reuter', 1)]\n",
      "[('notes', 2), ('pct', 2), ('commercial', 1), ('credit', 1), ('raising', 1), ('mln', 1), ('dlrs', 1), ('offering', 1), ('yielding', 1), ('said', 1), ('lead', 1), ('manager', 1), ('morgan', 1), ('stanley', 1), ('coupon', 1), ('priced', 1), ('yield', 1), ('basis', 1), ('points', 1), ('comparable', 1), ('treasury', 1), ('securities', 1), ('noncallable', 1), ('life', 1), ('issue', 1), ('rated', 1), ('baa', 1), ('moodys', 1), ('bbbplus', 1), ('standard', 1), ('poors', 1), ('boston', 1), ('corp', 1), ('shearson', 1), ('lehman', 1), ('brothers', 1), ('comanaged', 1), ('deal', 1), ('reuter', 1)]\n",
      "[('cattle', 3), ('experts', 2), ('campaign', 2), ('disease', 2), ('bangladesh', 2), ('india', 2), ('pakistan', 2), ('fao', 2), ('mln', 2), ('needed', 2), ('world', 1), ('animal', 1), ('health', 1), ('called', 1), ('eradicate', 1), ('lethal', 1), ('rinderpest', 1), ('bhutan', 1), ('nepal', 1), ('statement', 1), ('food', 1), ('agriculture', 1), ('organization', 1), ('meeting', 1), ('said', 1), ('dlrs', 1), ('years', 1), ('vaccinate', 1), ('entire', 1), ('susceptible', 1), ('population', 1), ('highrisk', 1), ('areas', 1), ('countries', 1), ('estimated', 1), ('risk', 1), ('recommended', 1), ('funded', 1), ('governments', 1), ('nations', 1), ('help', 1), ('similar', 1), ('campaigns', 1), ('egypt', 1), ('yemen', 1), ('iraq', 1), ('iran', 1), ('reuter', 1)]\n",
      "[('reserve', 2), ('corp', 2), ('said', 2), ('company', 2), ('oil', 1), ('minerals', 1), ('shareholders', 1), ('approved', 1), ('changing', 1), ('industries', 1), ('contracted', 1), ('purchase', 1), ('process', 1), ('recycle', 1), ('various', 1), ('waste', 1), ('materials', 1), ('generated', 1), ('ogden', 1), ('utah', 1), ('zirconium', 1), ('plant', 1), ('westinghouse', 1), ('electric', 1), ('corps', 1), ('wx', 1), ('western', 1), ('zieconium', 1), ('subsidiary', 1), ('reuter', 1)]\n",
      "     file_name  label deleted_stop_words\n",
      "0     2770.xml      0            1.80952\n",
      "1      812.xml      0            1.70732\n",
      "2      443.xml      0            1.79661\n",
      "3     1150.xml      0            1.44118\n",
      "4     1440.xml      0               None\n",
      "5     1138.xml      1               None\n",
      "6     4067.xml      1               None\n",
      "7     1748.xml      0               None\n",
      "8       91.xml      0               None\n",
      "9     2061.xml      1               None\n",
      "10    2524.xml      0               None\n",
      "11    4878.xml      0               None\n",
      "12    3271.xml      1               None\n",
      "13    1153.xml      0               None\n",
      "14    2767.xml      0               None\n",
      "15     172.xml      0               None\n",
      "16    4505.xml      1               None\n",
      "17     931.xml      0               None\n",
      "18    3033.xml      0               None\n",
      "19     619.xml      0               None\n",
      "20    4197.xml      0               None\n",
      "21    2060.xml      0               None\n",
      "22     370.xml      0               None\n",
      "23    4667.xml      0               None\n",
      "24    2987.xml      0               None\n",
      "25    4368.xml      0               None\n",
      "26    2035.xml      1               None\n",
      "27    4814.xml      1               None\n",
      "28    1285.xml      1               None\n",
      "29    3547.xml      0               None\n",
      "...        ...    ...                ...\n",
      "3570  1863.xml      0               None\n",
      "3571  3176.xml      0               None\n",
      "3572  3272.xml      0               None\n",
      "3573  2775.xml      0               None\n",
      "3574  4437.xml      1               None\n",
      "3575  3396.xml      0               None\n",
      "3576   161.xml      0               None\n",
      "3577  4554.xml      1               None\n",
      "3578  3846.xml      1               None\n",
      "3579  3299.xml      0               None\n",
      "3580   519.xml      0               None\n",
      "3581   375.xml      0               None\n",
      "3582  4237.xml      0               None\n",
      "3583   657.xml      0               None\n",
      "3584   122.xml      0               None\n",
      "3585  2096.xml      0               None\n",
      "3586  1841.xml      0               None\n",
      "3587  3710.xml      0               None\n",
      "3588  2161.xml      0               None\n",
      "3589  3781.xml      0               None\n",
      "3590  1478.xml      0               None\n",
      "3591  3729.xml      0               None\n",
      "3592  4584.xml      0               None\n",
      "3593   125.xml      0               None\n",
      "3594  1629.xml      0               None\n",
      "3595  4111.xml      1               None\n",
      "3596  4734.xml      0               None\n",
      "3597   610.xml      0               None\n",
      "3598  3395.xml      0               None\n",
      "3599  1278.xml      0               None\n",
      "\n",
      "[3600 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "reduce_item_number = 900 #1 = fullset ; 10 = 1/10 of the set\n",
    "\n",
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    common_terms = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        common_terms.append(word)\n",
    "    return common_terms\n",
    "\n",
    "class Type(Enum):\n",
    "    REAL_CSV_SETS = 1\n",
    "    TEST_ON_TRAINING_SET = 2\n",
    "\n",
    "class ArticlePredictorBase:\n",
    "    def __init__(self,test_type,path_to_csv_train,path_to_csv_test):\n",
    "        if Type.REAL_CSV_SETS == test_type:\n",
    "            \n",
    "            #Import of the test set\n",
    "            try:\n",
    "                test_csv_file = path_to_csv_test\n",
    "                test = pd.read_csv(test_csv_file,delimiter=',')\n",
    "                self.test_x = test.iloc[:,1]\n",
    "                self.test_x = test.iloc[:,2]\n",
    "            except:\n",
    "                print(\"Error while importing testing set csv file\") \n",
    "                self.type = None\n",
    "            \n",
    "            #Import of the train set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                train = pd.read_csv(train_csv_file,delimiter=',')\n",
    "                self.train_x = test.iloc[:,1]\n",
    "                self.train_y = test.iloc[:,2]\n",
    "            except:\n",
    "                print(\"Error while importing training set csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            self.type = test_type\n",
    "                \n",
    "        elif Type.TEST_ON_TRAINING_SET == test_type:\n",
    "            \n",
    "            #Import of the train/testing set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                data = pd.read_csv(train_csv_file,delimiter=',')\n",
    "            except:\n",
    "                print(\"Error while importing the csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            self.type = test_type\n",
    "                \n",
    "            X = data.iloc[:,1]\n",
    "            y = data.iloc[:,2]\n",
    "            \n",
    "            self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(X, y)\n",
    "        else:\n",
    "            self.type = None\n",
    "            print('Error type')\n",
    "\n",
    "    def preprocess(self, set_x, set_y):\n",
    "        the_set = pd.DataFrame()\n",
    "        the_set['file_name'] = list(set_x)\n",
    "        the_set['label'] = list(set_y)\n",
    "        the_set['deleted_stop_words'] = list(None for _ in range(the_set['file_name'].size))\n",
    "        \n",
    "        rover=0\n",
    "        for i in range(rover,the_set['file_name'].size//reduce_item_number):\n",
    "            # Path to the file of this element\n",
    "            file_path = 'project/project/data/'+str(the_set['file_name'][i])\n",
    "            # Except label\n",
    "            excepted = the_set['label'][i] \n",
    "            # Content of the BODY element in the file\n",
    "            itemlist = minidom.parse(file_path).getElementsByTagName('BODY')\n",
    "            \n",
    "            if len(itemlist) != 1:\n",
    "                print('Error: XML file invalid')\n",
    "\n",
    "            if itemlist[0].childNodes == []: #If the text is empty in the article\n",
    "                continue\n",
    "                \n",
    "            text = itemlist[0].childNodes[0].nodeValue\n",
    "            \n",
    "            #Preprocess part\n",
    "            \n",
    "            text = text.lower() #Lowercase\n",
    "            text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "            text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "            #Tokenization    \n",
    "            tokens = word_tokenize(text)\n",
    "            before_stop = len(tokens)\n",
    "            tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "            deleted_stop_words = before_stop/len(tokens)\n",
    "            \n",
    "            the_set['deleted_stop_words'][i] = before_stop/len(tokens)\n",
    "            \n",
    "            # Exemple pour récupérer les termes les plus fréquents\n",
    "            print(most_common_terms(tokens))\n",
    "            \n",
    "            \n",
    "            #TODO preprocess (compute des trucs, ajouter des champs dans le set comme plus haut (deleted_stop_words))\n",
    "            \n",
    "        print(the_set)\n",
    "        return set_x, the_set['label'] #TODO Renvoyer X avec tous les nouveaux fields\n",
    "            \n",
    "    def test(self):\n",
    "        print(\"(Testing Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            raise\n",
    "\n",
    "        test_x, test_y = self.preprocess(self.test_x, self.test_y)\n",
    "        #TODO testing\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"(Training Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            raise\n",
    "            \n",
    "        print(str(self.train_x.size)+\" elements in the training set\")\n",
    "        train_x, train_y = self.preprocess(self.train_x, self.train_y)\n",
    "        #TODO training\n",
    "      \n",
    "#predictor = ArticlePredictorBase(Type.REAL_CSV_SETS,\"project/project/train.csv\",\"project/project/test.csv\")\n",
    "predictor = ArticlePredictorBase(Type.TEST_ON_TRAINING_SET,\"project/project/train.csv\",None)\n",
    "predictor.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of 1 file\n",
    "\n",
    "We test here with 1.xml data file\n",
    "\n",
    "### Importation of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('project/project/data/1.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "\n",
    "#Verification\n",
    "\n",
    "if len(itemlist) != 1:\n",
    "    print('Error: XML file invalid')\n",
    "    sys.exit(0)\n",
    "\n",
    "text = itemlist[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess\n",
    "text = text.lower()    \n",
    "text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "#Preprocess ideas : https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "     \n",
    "#Tokenization    \n",
    "tokens = word_tokenize(text)\n",
    "print(str(len(tokens)) + \" tokens BEFORE stop words removing\")\n",
    "tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print(str(len(tokens)) + \" tokens AFTER stop words removing\")\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "#stemmer= PorterStemmer()\n",
    "#tokens = set(map(stemmer.stem,tokens))\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dlrs', 14)\n",
      "('new', 9)\n",
      "('york', 8)\n",
      "('sales', 7)\n",
      "('times', 7)\n",
      "('cocoa', 6)\n",
      "('comissaria', 5)\n",
      "('smith', 5)\n",
      "('said', 5)\n",
      "('bags', 5)\n",
      "('mln', 5)\n",
      "('crop', 5)\n",
      "('bahia', 4)\n",
      "('february', 3)\n",
      "('total', 3)\n",
      "('ports', 3)\n",
      "('junejuly', 3)\n",
      "('augsept', 3)\n",
      "('marchapril', 3)\n",
      "('octdec', 3)\n",
      "('dec', 3)\n",
      "('week', 2)\n",
      "('temporao', 2)\n",
      "('period', 2)\n",
      "('year', 2)\n",
      "('arrivals', 2)\n",
      "('kilos', 2)\n",
      "('consignment', 2)\n",
      "('figures', 2)\n",
      "('farmers', 2)\n",
      "('shippers', 2)\n",
      "('sold', 2)\n",
      "('bean', 2)\n",
      "('shipment', 2)\n",
      "('limited', 2)\n",
      "('tonne', 2)\n",
      "('open', 2)\n",
      "('july', 2)\n",
      "('butter', 2)\n",
      "('sept', 2)\n",
      "('currency', 2)\n",
      "('areas', 2)\n",
      "('uruguay', 2)\n",
      "('showers', 1)\n",
      "('continued', 1)\n",
      "('zone', 1)\n",
      "('alleviating', 1)\n",
      "('drought', 1)\n",
      "('early', 1)\n",
      "('january', 1)\n",
      "('improving', 1)\n",
      "('prospects', 1)\n",
      "('coming', 1)\n",
      "('normal', 1)\n",
      "('humidity', 1)\n",
      "('levels', 1)\n",
      "('restored', 1)\n",
      "('weekly', 1)\n",
      "('review', 1)\n",
      "('dry', 1)\n",
      "('means', 1)\n",
      "('late', 1)\n",
      "('ended', 1)\n",
      "('making', 1)\n",
      "('cumulative', 1)\n",
      "('season', 1)\n",
      "('stage', 1)\n",
      "('delivered', 1)\n",
      "('earlier', 1)\n",
      "('included', 1)\n",
      "('doubt', 1)\n",
      "('old', 1)\n",
      "('available', 1)\n",
      "('harvesting', 1)\n",
      "('practically', 1)\n",
      "('come', 1)\n",
      "('end', 1)\n",
      "('estimates', 1)\n",
      "('standing', 1)\n",
      "('thousand', 1)\n",
      "('hands', 1)\n",
      "('middlemen', 1)\n",
      "('exporters', 1)\n",
      "('processors', 1)\n",
      "('doubts', 1)\n",
      "('fit', 1)\n",
      "('export', 1)\n",
      "('experiencing', 1)\n",
      "('dificulties', 1)\n",
      "('obtaining', 1)\n",
      "('superior', 1)\n",
      "('certificates', 1)\n",
      "('view', 1)\n",
      "('lower', 1)\n",
      "('quality', 1)\n",
      "('recent', 1)\n",
      "('weeks', 1)\n",
      "('good', 1)\n",
      "('held', 1)\n",
      "('spot', 1)\n",
      "('prices', 1)\n",
      "('rose', 1)\n",
      "('cruzados', 1)\n",
      "('arroba', 1)\n",
      "('reluctant', 1)\n",
      "('offer', 1)\n",
      "('nearby', 1)\n",
      "('booked', 1)\n",
      "('march', 1)\n",
      "('named', 1)\n",
      "('light', 1)\n",
      "('going', 1)\n",
      "('fob', 1)\n",
      "('routine', 1)\n",
      "('aprilmay', 1)\n",
      "('went', 1)\n",
      "('destinations', 1)\n",
      "('covertible', 1)\n",
      "('cake', 1)\n",
      "('registered', 1)\n",
      "('aug', 1)\n",
      "('buyers', 1)\n",
      "('argentina', 1)\n",
      "('convertible', 1)\n",
      "('liquor', 1)\n",
      "('selling', 1)\n",
      "('currently', 1)\n",
      "('estimated', 1)\n",
      "('final', 1)\n",
      "('expected', 1)\n",
      "('published', 1)\n",
      "('brazilian', 1)\n",
      "('trade', 1)\n",
      "('commission', 1)\n",
      "('carnival', 1)\n",
      "('ends', 1)\n",
      "('midday', 1)\n",
      "('reuter', 1)\n"
     ]
    }
   ],
   "source": [
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        print(word)\n",
    "        \n",
    "most_common_terms(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
