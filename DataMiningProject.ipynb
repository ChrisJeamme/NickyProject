{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator, re, string, codecs, nltk\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "from xml.dom import minidom\n",
    "from string import punctuation\n",
    "from enum import Enum\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    from string import maketrans\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Test\n",
    "\n",
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import xgboost, textblob\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Launched)\n",
      "3600 elements in the training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     file_name  label deleted_stop_words\n",
      "0     3195.xml      0            1.68966\n",
      "1     3349.xml      0                1.4\n",
      "2     2619.xml      1            1.18182\n",
      "3      847.xml      0            1.64444\n",
      "4      343.xml      0            1.64286\n",
      "5     1623.xml      0            1.88636\n",
      "6     3959.xml      0               None\n",
      "7     2781.xml      1                1.4\n",
      "8     1624.xml      0            1.65517\n",
      "9     2604.xml      0                1.5\n",
      "10    3029.xml      0            1.85057\n",
      "11     870.xml      0            1.39535\n",
      "12    3779.xml      0            1.69343\n",
      "13     652.xml      0             2.0625\n",
      "14    2862.xml      0            1.67442\n",
      "15    2774.xml      0               1.75\n",
      "16     955.xml      0            1.68553\n",
      "17    2539.xml      1            1.22807\n",
      "18    3137.xml      0            1.81548\n",
      "19    1084.xml      1            1.35075\n",
      "20     649.xml      0              1.125\n",
      "21     438.xml      1            1.09091\n",
      "22    4101.xml      0            1.92488\n",
      "23    4246.xml      0            1.63054\n",
      "24    2066.xml      0             1.7234\n",
      "25    3945.xml      0            1.52632\n",
      "26    2454.xml      0            1.47059\n",
      "27    1614.xml      0            1.72414\n",
      "28    1947.xml      0            1.66901\n",
      "29    3185.xml      0            1.91489\n",
      "...        ...    ...                ...\n",
      "3570    44.xml      0               None\n",
      "3571  3801.xml      1               None\n",
      "3572  1451.xml      0               None\n",
      "3573  1148.xml      0               None\n",
      "3574  2291.xml      0               None\n",
      "3575  4137.xml      0               None\n",
      "3576   243.xml      0               None\n",
      "3577  3545.xml      1               None\n",
      "3578   486.xml      0               None\n",
      "3579  1508.xml      1               None\n",
      "3580   530.xml      1               None\n",
      "3581  2358.xml      0               None\n",
      "3582  4779.xml      1               None\n",
      "3583  1718.xml      1               None\n",
      "3584   260.xml      0               None\n",
      "3585   319.xml      0               None\n",
      "3586   765.xml      0               None\n",
      "3587  1066.xml      1               None\n",
      "3588  4867.xml      0               None\n",
      "3589   124.xml      0               None\n",
      "3590  3125.xml      0               None\n",
      "3591  3664.xml      0               None\n",
      "3592  2674.xml      1               None\n",
      "3593  3060.xml      0               None\n",
      "3594  4721.xml      0               None\n",
      "3595  3752.xml      0               None\n",
      "3596  3428.xml      0               None\n",
      "3597  3558.xml      0               None\n",
      "3598  3893.xml      1               None\n",
      "3599  4627.xml      0               None\n",
      "\n",
      "[3600 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "reduce_item_number = 100 #1 = fullset ; 10 = 1/10 of the set\n",
    "\n",
    "class Type(Enum):\n",
    "    REAL_CSV_SETS = 1\n",
    "    TEST_ON_TRAINING_SET = 2\n",
    "\n",
    "class ArticlePredictorBase:\n",
    "    def __init__(self,test_type,path_to_csv_train,path_to_csv_test):\n",
    "        if Type.REAL_CSV_SETS == test_type:\n",
    "            \n",
    "            #Import of the test set\n",
    "            try:\n",
    "                test_csv_file = path_to_csv_test\n",
    "                test = pd.read_csv(test_csv_file,delimiter=',')\n",
    "                self.test_x = test.iloc[:,1]\n",
    "                self.test_x = test.iloc[:,2]\n",
    "            except:\n",
    "                print(\"Error while importing testing set csv file\") \n",
    "                self.type = None\n",
    "            \n",
    "            #Import of the train set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                train = pd.read_csv(train_csv_file,delimiter=',')\n",
    "                self.train_x = test.iloc[:,1]\n",
    "                self.train_y = test.iloc[:,2]\n",
    "            except:\n",
    "                print(\"Error while importing training set csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            self.type = test_type\n",
    "                \n",
    "        elif Type.TEST_ON_TRAINING_SET == test_type:\n",
    "            \n",
    "            #Import of the train/testing set\n",
    "            try:\n",
    "                train_csv_file = path_to_csv_train\n",
    "                data = pd.read_csv(train_csv_file,delimiter=',')\n",
    "            except:\n",
    "                print(\"Error while importing the csv file\") \n",
    "                self.type = None\n",
    "                \n",
    "            self.type = test_type\n",
    "                \n",
    "            X = data.iloc[:,1]\n",
    "            y = data.iloc[:,2]\n",
    "            \n",
    "            self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(X, y)\n",
    "        else:\n",
    "            self.type = None\n",
    "            print('Error type')\n",
    "\n",
    "    def preprocess(self, set_x, set_y):\n",
    "        the_set = pd.DataFrame()\n",
    "        the_set['file_name'] = list(set_x)\n",
    "        the_set['label'] = list(set_y)\n",
    "        the_set['deleted_stop_words'] = [None for _ in range(the_set['file_name'].size)]\n",
    "        \n",
    "        rover=0\n",
    "        for i in range(rover,the_set['file_name'].size//reduce_item_number):\n",
    "            # Path to the file of this element\n",
    "            file_path = 'project/project/data/'+str(the_set['file_name'][i])\n",
    "            # Except label\n",
    "            excepted = the_set['label'][i] \n",
    "            # Content of the BODY element in the file\n",
    "            itemlist = minidom.parse(file_path).getElementsByTagName('BODY')\n",
    "            \n",
    "            if len(itemlist) != 1:\n",
    "                print('Error: XML file invalid')\n",
    "\n",
    "            if itemlist[0].childNodes == []: #If the text is empty in the article\n",
    "                continue\n",
    "                \n",
    "            text = itemlist[0].childNodes[0].nodeValue\n",
    "            \n",
    "            #Preprocess part\n",
    "            \n",
    "            text = text.lower() #Lowercase\n",
    "            text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "            text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "            #Tokenization    \n",
    "            tokens = word_tokenize(text)\n",
    "            before_stop = len(tokens)\n",
    "            tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "            deleted_stop_words = before_stop/len(tokens)\n",
    "            \n",
    "            the_set['deleted_stop_words'][i] = before_stop/len(tokens)\n",
    "        print(the_set)\n",
    "        \n",
    "            #TODO preprocess (accès au fichier, compute des trucs, ajouter des champs dans le set)\n",
    "            \n",
    "        return set_x, set_y\n",
    "            \n",
    "    def test(self):\n",
    "        print(\"(Testing Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            raise\n",
    "\n",
    "        test_x, test_y = self.preprocess(self.test_x, self.test_y)\n",
    "        #TODO testing\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"(Training Launched)\")\n",
    "        if self.type is None:\n",
    "            print(\"Initialisation error \")\n",
    "            raise\n",
    "            \n",
    "        print(str(self.train_x.size)+\" elements in the training set\")\n",
    "        train_x, train_y = self.preprocess(self.train_x, self.train_y)\n",
    "        #TODO training\n",
    "      \n",
    "#predictor = ArticlePredictorBase(Type.REAL_CSV_SETS,\"project/project/train.csv\",\"project/project/test.csv\")\n",
    "predictor = ArticlePredictorBase(Type.TEST_ON_TRAINING_SET,\"project/project/train.csv\",None)\n",
    "predictor.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of 1 file\n",
    "\n",
    "We test here with 1.xml data file\n",
    "\n",
    "### Importation of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('project/project/data/1.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('BODY')\n",
    "\n",
    "#Verification\n",
    "\n",
    "if len(itemlist) != 1:\n",
    "    print('Error: XML file invalid')\n",
    "    sys.exit(0)\n",
    "\n",
    "text = itemlist[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess\n",
    "text = text.lower()    \n",
    "text = re.sub(r'\\d+', '', text) #Deleting numbers\n",
    "text = text.translate(str.maketrans('','',string.punctuation)) #Deleting ponctuation\n",
    "\n",
    "#Preprocess ideas : https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "     \n",
    "#Tokenization    \n",
    "tokens = word_tokenize(text)\n",
    "print(str(len(tokens)) + \" tokens BEFORE stop words removing\")\n",
    "tokens = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print(str(len(tokens)) + \" tokens AFTER stop words removing\")\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "#stemmer= PorterStemmer()\n",
    "#tokens = set(map(stemmer.stem,tokens))\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dlrs', 14)\n",
      "('new', 9)\n",
      "('york', 8)\n",
      "('sales', 7)\n",
      "('times', 7)\n",
      "('cocoa', 6)\n",
      "('comissaria', 5)\n",
      "('smith', 5)\n",
      "('said', 5)\n",
      "('bags', 5)\n",
      "('mln', 5)\n",
      "('crop', 5)\n",
      "('bahia', 4)\n",
      "('february', 3)\n",
      "('total', 3)\n",
      "('ports', 3)\n",
      "('junejuly', 3)\n",
      "('augsept', 3)\n",
      "('marchapril', 3)\n",
      "('octdec', 3)\n",
      "('dec', 3)\n",
      "('week', 2)\n",
      "('temporao', 2)\n",
      "('period', 2)\n",
      "('year', 2)\n",
      "('arrivals', 2)\n",
      "('kilos', 2)\n",
      "('consignment', 2)\n",
      "('figures', 2)\n",
      "('farmers', 2)\n",
      "('shippers', 2)\n",
      "('sold', 2)\n",
      "('bean', 2)\n",
      "('shipment', 2)\n",
      "('limited', 2)\n",
      "('tonne', 2)\n",
      "('open', 2)\n",
      "('july', 2)\n",
      "('butter', 2)\n",
      "('sept', 2)\n",
      "('currency', 2)\n",
      "('areas', 2)\n",
      "('uruguay', 2)\n",
      "('showers', 1)\n",
      "('continued', 1)\n",
      "('zone', 1)\n",
      "('alleviating', 1)\n",
      "('drought', 1)\n",
      "('early', 1)\n",
      "('january', 1)\n",
      "('improving', 1)\n",
      "('prospects', 1)\n",
      "('coming', 1)\n",
      "('normal', 1)\n",
      "('humidity', 1)\n",
      "('levels', 1)\n",
      "('restored', 1)\n",
      "('weekly', 1)\n",
      "('review', 1)\n",
      "('dry', 1)\n",
      "('means', 1)\n",
      "('late', 1)\n",
      "('ended', 1)\n",
      "('making', 1)\n",
      "('cumulative', 1)\n",
      "('season', 1)\n",
      "('stage', 1)\n",
      "('delivered', 1)\n",
      "('earlier', 1)\n",
      "('included', 1)\n",
      "('doubt', 1)\n",
      "('old', 1)\n",
      "('available', 1)\n",
      "('harvesting', 1)\n",
      "('practically', 1)\n",
      "('come', 1)\n",
      "('end', 1)\n",
      "('estimates', 1)\n",
      "('standing', 1)\n",
      "('thousand', 1)\n",
      "('hands', 1)\n",
      "('middlemen', 1)\n",
      "('exporters', 1)\n",
      "('processors', 1)\n",
      "('doubts', 1)\n",
      "('fit', 1)\n",
      "('export', 1)\n",
      "('experiencing', 1)\n",
      "('dificulties', 1)\n",
      "('obtaining', 1)\n",
      "('superior', 1)\n",
      "('certificates', 1)\n",
      "('view', 1)\n",
      "('lower', 1)\n",
      "('quality', 1)\n",
      "('recent', 1)\n",
      "('weeks', 1)\n",
      "('good', 1)\n",
      "('held', 1)\n",
      "('spot', 1)\n",
      "('prices', 1)\n",
      "('rose', 1)\n",
      "('cruzados', 1)\n",
      "('arroba', 1)\n",
      "('reluctant', 1)\n",
      "('offer', 1)\n",
      "('nearby', 1)\n",
      "('booked', 1)\n",
      "('march', 1)\n",
      "('named', 1)\n",
      "('light', 1)\n",
      "('going', 1)\n",
      "('fob', 1)\n",
      "('routine', 1)\n",
      "('aprilmay', 1)\n",
      "('went', 1)\n",
      "('destinations', 1)\n",
      "('covertible', 1)\n",
      "('cake', 1)\n",
      "('registered', 1)\n",
      "('aug', 1)\n",
      "('buyers', 1)\n",
      "('argentina', 1)\n",
      "('convertible', 1)\n",
      "('liquor', 1)\n",
      "('selling', 1)\n",
      "('currently', 1)\n",
      "('estimated', 1)\n",
      "('final', 1)\n",
      "('expected', 1)\n",
      "('published', 1)\n",
      "('brazilian', 1)\n",
      "('trade', 1)\n",
      "('commission', 1)\n",
      "('carnival', 1)\n",
      "('ends', 1)\n",
      "('midday', 1)\n",
      "('reuter', 1)\n"
     ]
    }
   ],
   "source": [
    "def most_common_terms(terms):\n",
    "    terms_count_array = []\n",
    "    for term in terms:\n",
    "        terms_count_array += term.split(\" \")\n",
    "    counter = Counter(terms_count_array)\n",
    "    for word in counter.most_common():\n",
    "        print(word)\n",
    "        \n",
    "most_common_terms(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
